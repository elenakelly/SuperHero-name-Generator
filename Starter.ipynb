{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data and Tokenizer\n",
    "1. Import the data\n",
    "2. Create a tokenizer\n",
    "3. Char to index and Index to char dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'superhero'...\n",
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 8 (delta 0), reused 4 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (8/8), 47.08 KiB | 1.88 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/am1tyadav/superhero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumpa\\t\\ndoctor fate\\t\\nstarlight\\t\\nisildur\\t\\nlasher\\t\\nvarvara\\t\\nthe target\\t\\naxel\\t\\nbattra\\t\\nchangeling\\t\\npyrrh'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('superhero/superheroes.txt','r') as f:\n",
    "    data = f.read()\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~',\n",
    "    split='\\n',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '\\t', 2: 'a', 3: 'e', 4: 'r', 5: 'o', 6: 'n', 7: 'i', 8: ' ', 9: 't', 10: 's', 11: 'l', 12: 'm', 13: 'h', 14: 'd', 15: 'c', 16: 'u', 17: 'g', 18: 'k', 19: 'b', 20: 'p', 21: 'y', 22: 'w', 23: 'f', 24: 'v', 25: 'j', 26: 'z', 27: 'x', 28: 'q'}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = tokenizer.word_index\n",
    "index_to_char = dict((v,k) for k , v in char_to_index.items())\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Names and Sequences\n",
    "1. Converting between names and sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jumpa\\t',\n",
       " 'doctor fate\\t',\n",
       " 'starlight\\t',\n",
       " 'isildur\\t',\n",
       " 'lasher\\t',\n",
       " 'varvara\\t',\n",
       " 'the target\\t',\n",
       " 'axel\\t',\n",
       " 'battra\\t',\n",
       " 'changeling\\t']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = data.splitlines()\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25], [16], [12], [20], [2], [1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_seq(name):\n",
    "    return[tokenizer.texts_to_sequences(c)[0][0] for c in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 16, 12, 20, 2, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_seq(names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_name(seq):\n",
    "    return ''.join([index_to_char[i] for i in seq if i != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumpa\\t'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_to_name(name_to_seq(names[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doctor fate\t\n",
      "[[14], [5], [15], [9], [5], [4], [8], [23], [2], [9], [3], [1]]\n",
      "[14, 5, 15, 9, 5, 4, 8, 23, 2, 9, 3, 1]\n",
      "doctor fate\t\n"
     ]
    }
   ],
   "source": [
    "print(names[1])\n",
    "print(tokenizer.texts_to_sequences(names[1]))\n",
    "print(name_to_seq(names[1]))\n",
    "print(seq_to_name(name_to_seq(names[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Creating Examples\n",
    "1. Creating sequences\n",
    "2. Padding all sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences= []\n",
    "for name in names:\n",
    "    seq = name_to_seq(name)\n",
    "    if len(seq) >=2:\n",
    "        sequences += [seq[:i] for i in  range(2, len(seq) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 5],\n",
       " [14, 5, 15],\n",
       " [14, 5, 15, 9],\n",
       " [14, 5, 15, 9, 5],\n",
       " [14, 5, 15, 9, 5, 4],\n",
       " [14, 5, 15, 9, 5, 4, 8],\n",
       " [14, 5, 15, 9, 5, 4, 8, 23],\n",
       " [14, 5, 15, 9, 5, 4, 8, 23, 2],\n",
       " [14, 5, 15, 9, 5, 4, 8, 23, 2, 9],\n",
       " [14, 5, 15, 9, 5, 4, 8, 23, 2, 9, 3],\n",
       " [14, 5, 15, 9, 5, 4, 8, 23, 2, 9, 3, 1]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[5:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(x) for x in sequences])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 25 16 12]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, \n",
    "    padding='pre',\n",
    "    maxlen= max_len\n",
    ")\n",
    "print(padded_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88279, 33)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Training and Validation Sets\n",
    "1. Creating training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Creating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Generating Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "072a2607209b90e63776b11336ea1c1100515cac8f56f8a8d9a008c012dd7728"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
